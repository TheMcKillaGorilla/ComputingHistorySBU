<html>

<head>
<title>Tech History Web Page</title>
<style>
    h1 {text-align: center;}
    .myDiv {
        border: 5px outset black;
        background-color: lightblue;
        text-align: center;
    }
    .center {
        display: block;
        margin-left: auto;
        margin-right: auto;
    }
</style>
</head>

<body>
<div class="myDiv">
    <h1 style = "font-family: 'Times New Roman', Times, serif; font-size: 250%; font-weight: bold;">History of the CPU</h1>
</div>
<p></p>
<img src= "CPU Timeline.png" alt="CPU Timeline" width="1888" height="400">
<h1 style = "font-family: 'Times New Roman', Times, serif; font-size: 150%; font-weight: bold;"><u>WHAT IS A CPU?</u></h1>
<p>
    The CPU or the central processing unit is often known as the brain of the modern computer. 
    It contains the circuits that execute the instructions of a program and manages the rest of the computer parts in the system. 
    While the CPU is possibly the most important part of our computer today, this piece of hardware has change drastically over
    time since it was initially created. While it is important to understand how we got to the modern CPUs that we use in all our
    computers today, we should also look at what led to the creation of the CPU.
</p>
<img src= "CPU.jpg" alt="CPU" width="500" height="400" class="center">
<p></p>
<h1 style = "font-family: 'Times New Roman', Times, serif; font-size: 150%; font-weight: bold;"><u>BEFORE THE CPU</u></h1>
<p>
    Before the CPU or even the first computer, humanity relied on sheer willpower and tools to do calculations.
    While this worked for thousands of years, as problems became more complicated and more calculations needed
    to be done, humanity started to try to find ways to create machines that would help with the calculations.
    As these machines started to evolve, they eventually lead to the creation of the computer and with that, the CPU.
</p>
<h2 style = "font-family: 'Times New Roman', Times, serif; font-size: 100%; font-weight: bold;">Babbage Difference Engine</h2>
<p>
    One of the first machines that led to the creation of the modern computer was the difference engine,
    a calculating machine designed in the 1820s <a href="https://en.wikipedia.org/wiki/Difference_engine">[4]</a>,
    created by Charles Babbage. This machine was Babbage’s solution to help make calculations for the nautical tables
    easier and that interested the British government greatly. The government decided to fund the project and Babbage
    started to build the machine however, as Babbage worked on the machine, he thought of a better machine that would
    be a general-purpose version of the difference engine and called it the analytical engine. The analytical engine
    ended up taking all of Babbage’s attention which led to the difference engine never being completed. As a result,
    the British government stopped funding Babbage’s research which led to him not being able to finish the analytical
    engine as well. While the difference engine was more like a modern calculator than a modern computer, it led Babbage
    to the idea of a general-purpose machine in the analytical engine, an important concept that led to the creation
    of the modern computer. Babbage is a good example of a computer pioneer that was far ahead of their time causing
    their project to fail.
</p>
<img src= "Difference Engine.jpg" alt="Difference Engine" width="500" height="400" class="center">
<h2 style = "font-family: 'Times New Roman', Times, serif; font-size: 100%; font-weight: bold;">Harvard Mark 1</h2>
<p>
    Around a hundred years after Babbage, a man known as Howard Aiken would take concepts from Babbage’s analytical
    engine and apply them to his own calculator called the IBM Automatic Sequence Controlled Calculator (ASCC) or
    the Mark 1. While Babbage never finished the analytical engine, in February 1944, IBM engineers finished building
    the Mark 1 and had it shipped to Harvard <a href="https://en.wikipedia.org/wiki/Harvard_Mark_I">[6]</a>. This
    machine was used to help the Navy during the last parts of World War 2 and then was later disassembled in 1959.
    Thanks to this machine, it helped shape the development of computers later by introducing many important computer
    pioneers to the concept of a computer such as Grace Brewster Murray Hopper, one of the first computer programmers,
    and IBM, one of the largest computer hardware companies before the personal computer. The creation of the Harvard
    Mark 1 was another step towards what we know now as a computer and the CPU inside the computer.
</p>
<img src= "Mark 1.jpg" alt="Harvard Mark 1" width="500" height="400" class="center">
<h2 style = "font-family: 'Times New Roman', Times, serif; font-size: 100%; font-weight: bold;">ENIAC</h2>
<p>
    Around the same time as the Harvard Mark 1, at the University of Pennsylvania, John Mauchly and J. Presper Eckert
    would start working on the first programmable, electronic, general-purpose digital computer known as the Electronic
    Numerical Integrator and Computer or ENIAC <a href="https://en.wikipedia.org/wiki/ENIAC#:~:text=ENIAC%20(%2F%CB%88%
    C9%9Bni,of%20numerical%20problems%22%20through%20reprogramming">[5]</a>. Completed in 1945, this machine is generally
    credited as the first computer, being used primarily to calculate artillery firing tables for the US army. While the
    ENIAC was able to do calculations really quicky for its time, it was still missing many parts we normally expect to
    see in a computer today, one being the CPU. While this machine was a huge milestone in computing history, it was also
    quickly replaced as other pioneers tried to improve on the design and made computers such as the EDSAC and the
    Manchester Baby. This computer help set the path for modern computers and helped fuel the demand for faster and better
    computers.
</p>
<img src= "ENIAC.jpg" alt="ENIAC" width="500" height="400" class="center">
<h2 style = "font-family: 'Times New Roman', Times, serif; font-size: 100%; font-weight: bold;">Von Neumann Architecture</h2>
<p>
    One important figure that was part of the ENIAC launch was John von Neumann. After unveiling the ENIAC, von Neumann
    and the creators of the ENIAC were part of a series of lectures, called the Moore School Lectures, that discussed
    what improvements can be made to the ENIAC. One such improvements was the idea of implementing a von Neumann
    architecture, one of the most important concepts that we still use in modern CPUs today, into the new machines.
    The von Neumann architecture describes an electronic digital computer with a processing unit that has an arithmetic
    logic unit (ALU) and processor registers, a control unit with an instruction register and program counter, a memory
    that stores data and instructions, external storage, and input and output mechanisms <a href="https://en.wikipedia.org/wiki/
    Von_Neumann_architecture">[12]</a>. While the first CPU did not exist till about 30 years after this concept was
    shared at the Moore School Lectures, it was a significant step in how we think about computer architecture and would
    later be developed further to create the modern CPUs we use today.
</p>
<img src= "Von Neumann Architecture.png" alt="Von Neumann Architecture" width="500" height="400" class="center">
<h2 style = "font-family: 'Times New Roman', Times, serif; font-size: 100%; font-weight: bold;">Time-sharing</h2>
<p>
    One of the last computer concepts before the idea of a personal computer was created and took over the
    computing market was time-sharing. Time-sharing is the idea of using one computer that was attached to multiple
    terminals to serve multiple people at the same time. The computer would be able to accomplish this by dividing
    up resources and doing tasks for each terminal for a certain amount of time before going to the next terminal.
    This would make it so if there were only a few terminals active, the people at the terminals would feel like they
    had the computer all to themselves but as more terminals were active, it would slow down the performance for
    everyone. One of the first time sharing operating systems was the Compatible Time-Sharing System (CTSS) and it was
    first demonstrated on MIT’s IBM 709 on November 1961 <a href="https://en.wikipedia.org/wiki/Compatible_Time-Sharing
    _System">[3]</a>. Time-sharing allowed more people to get their hands on a computer and learn how to use and
    eventually make their own programs for the computer. Thanks to the new interest generated from people getting hands
    on experience with the computer due to time-sharing, this would drive the excitement for personal computers later
    down the road and make computers more available to the general population.
</p>
<img src= "Time Sharing.png" alt="Time-Sharing" width="700" height="400" class="center">
<p></p>
<h1 style = "font-family: 'Times New Roman', Times, serif; font-size: 150%; font-weight: bold;"><u>CREATION OF THE CPU</u></h1>
<p>
    Before the creation of the personal computer, the only people that had access to computers were the large
    corporations and universities that had large mainframe computers. After the creation of the first microprocessor
    or CPU, the creation of smaller less powerful but personalized computers was possible and these computers were also
    affordable for the public to buy. This shift was not that significant at first, but as personal computers got more
    powerful, they started to replace mainframe computer as the norm. Without the creation of the first microprocessor
    or CPU, what we think of as a computer today may be significantly different from what we have.
</p>
<h2 style = "font-family: 'Times New Roman', Times, serif; font-size: 100%; font-weight: bold;">Intel 4004</h2>
<p>
    Before the personal computer, many of the computers that were active were large mainframe computers that were
    expensive to buy and own. Before the creation of the personal computer was possible, the parts for these computers
    needed to be created first. One of the most important parts for the personal computer is the CPU. The first
    microprocessor or CPU was created by Intel on November 15, 1971 called the Intel 4004 <a href="https://en.wikipedia.org/wiki/
    Intel_4004">[8]</a>. This CPU was created due to a request by Busicom Corp to Intel to create a chip for
    their calculator. While this CPU was not used to make a personal computer, Intel would continue to make CPUs over
    time, slowly improving on it, until they made their first CPU that was used in the Altair 8800, the first true
    personal computer <a href="https://en.wikipedia.org/wiki/Personal_computer#History">[11]</a>. The Intel 4004
    launched Intel into the CPU making business, which they are still part of to this day, and made personal computers
    possible.
</p>
<img src= "Intel 4004.jfif" alt="Intel 4004" width="500" height="400" class="center">
<h2 style = "font-family: 'Times New Roman', Times, serif; font-size: 100%; font-weight: bold;">Altair 8800</h2>
<p>
    After the first microprocessor was created, Intel continued to improve the CPU making it more and more powerful overtime.
    Although these CPUs were not anywhere close to being as powerful as the mainframe computers that companies such as IBM
    were making, some companies saw an opportunity in going for a different group of consumers, one that IBM was not aiming at.
    This led to the first personal computer being made by Micro Instrumentation and Telemetry Systems or MITS called the
    Altair 8800. This computer used the Intel 8080 CPU and was a do-it-yourself kit aim towards hobbyist
    <a href="https://en.wikipedia.org/wiki/Altair_8800">[1]</a>. Thanks to this personal computer, it shaped the computing
    industry by helping Bill Gates, one of the founders of Microsoft, enter the computing scene with him creating Altair BASIC
    as one of Microsoft’s first products. This computer also influenced people such as Steve Wozniak to build their own computer
    leading to the creation of Apple. Thanks to the invention of the CPU, it led to the Altair 8800 and that helped start
    some of the largest tech companies we have today.
</p>
<img src= "Altair 8800.jpg" alt="Altair 8800" width="500" height="400" class="center">
<h2 style = "font-family: 'Times New Roman', Times, serif; font-size: 100%; font-weight: bold;">Improvements Over Time</h2>
<p>
    As the personal computer started getting more and more popular, it started to replace mainframe computer. Another effect
    that happened overtime was that the number of transistors in the integrated circuit started to multiple, which Gordon Moore
    observed leading to the observation to be called Moore’s Law <a href="https://en.wikipedia.org/wiki/Moore%27s_law">[10]</a>.
    This led to cheaper and more powerful CPUs over time which led to ideas such as instruction pipelining, multi-threading and
    multi-core to emerged to further improve the processing power of the CPU. Instruction pipelining was the idea of making CPUs
    work on multiple instruction sets that are in different stages at the same time, this would allow the CPU to be more efficient
    so that all parts of the CPU is working rather than waiting for one instruction set to be fully complete before starting another
    <a href="https://en.wikipedia.org/wiki/History_of_general-purpose_CPUs#Mid-to-late_1980s:_Exploiting_instruction_level_
    parallelism">[7]</a>. The idea of multi-threading made it so CPUs would be able to handle multiple programs running at the same
    time <a href="https://en.wikipedia.org/wiki/History_of_general-purpose_CPUs#Mid-to-late_1980s:_Exploiting_instruction_level_
    parallelism">[7]</a>. The idea of multi-core made it so one CPU would contain multiple processing units working at the same time
    to complete the program <a href="https://en.wikipedia.org/wiki/History_of_general-purpose_CPUs#Mid-to-late_1980s:_Exploiting_
    instruction_level_parallelism">[7]</a>. With these improvements overtime, the speed that CPUs could execute their instruction
    sets now are significantly faster than the first CPUs created.
</p>
<p></p>
<h1 style = "font-family: 'Times New Roman', Times, serif; font-size: 150%; font-weight: bold;"><u>MODERN AGE OF THE CPU</u></h1>
<p>
    Since the creation of the CPU, it has come a long way and now, it is in almost every piece of technology we use today.
    Due to the introduction of new pieces of technology, the companies that started making CPUs back in the 1970s are still
    improving on them every year making them newer and faster while some new companies join the market after.
</p>
<h2 style = "font-family: 'Times New Roman', Times, serif; font-size: 100%; font-weight: bold;">CPU Market</h2>
<p>
    Today, we still have Intel and AMD competing head and head every year to make the best new CPU for personal computers,
    while Intel has controlled most of the market share for many of the years since they created the first CPU, AMD has recently
    started to close the gap and as of January 5, 2021, Intel controlled 50.2% of the market share while AMD controlled 49.8%
    <a href="https://www.zdnet.com/article/amd-closing-in-on-intel-in-desktop-cpu-market-share/#:~:text=For%20the%20first%20
    time%20since,with%20Intel%20at%2050.2%20percent">[9]</a>. While these two companies were competing, Apple started the
    development of their own CPUs for their personal computers, with the M1 chip released on November 10, 2020 for the MacBook
    Air, Mac Mini, and MacBook Pro <a href="https://en.wikipedia.org/wiki/Apple_M1">[2]</a>. Another CPU that is important in
    modern society is the CPU in the mobile phone. Smartphones has become more and more powerful thanks to the improvements to
    its CPU over time, and this is mainly thanks to the improvements to the Qualcomm CPUs most Android phones use and A series
    CPUs Apple uses. While CPUs have changed significantly since their initial creation, we have not seen as many significant
    improvements as we have seen in the early stages of the CPU.
</p>
<img src= "Apple M1.jpg" alt="Apple M1" width="700" height="400" class="center">
<h2 style = "font-family: 'Times New Roman', Times, serif; font-size: 100%; font-weight: bold;">Quantum Computing</h2>
<p>
    While the improvement of the CPU over time has started to slow down due to the transistors being too small now, we are looking
    towards other methods of increasing the processing power of our computers. One idea for the next step of computing is quantum
    computing, these computers are theorized to be far more powerful than the computers we have right now and companies such as IBM,
    Google, and Amazon are investing millions of dollars to start the development of this emerging field. While the quantum computer
    is still in its infancy and we are still far from making them accessible to the public, it is now our turn to be the John Mauchly
    and J. Presper Eckert of our time and make strides in quantum computing for the future to build upon. Maybe one day, sometime in
    the future, quantum computers will dominate the computing market and make our modern computers obsolete.
</p>
<img src= "Quantum Computer.jpg" alt="Quantum Computer" width="700" height="400" class="center">
<p></p>
<h1 style = "font-family: 'Times New Roman', Times, serif; font-size: 100%; font-weight: bold;"><u>Sources</u></h1>
<p>
    <a href="https://en.wikipedia.org/wiki/Altair_8800">[1]</a> “Altair 8800.” Wikipedia, Wikimedia Foundation, 28 Dec. 2020,
    en.wikipedia.org/wiki/Altair_8800. 
</p>
<p>
    <a href="https://en.wikipedia.org/wiki/Apple_M1">[2]</a> “Apple M1.” Wikipedia, Wikimedia Foundation, 12 Jan. 2021,
    en.wikipedia.org/wiki/Apple_M1. 
</p>
<p>
    <a href="https://en.wikipedia.org/wiki/Compatible_Time-Sharing_System">[3]</a> “Compatible Time-Sharing System.” Wikipedia,
    Wikimedia Foundation, 22 Nov. 2020, en.wikipedia.org/wiki/Compatible_Time-Sharing_System. 
</p>
<p>
    <a href="https://en.wikipedia.org/wiki/Difference_engine">[4]</a> “Difference Engine.” Wikipedia, Wikimedia Foundation,
    11 Jan. 2021, en.wikipedia.org/wiki/Difference_engine. 
</p>
<p>
    <a href="https://en.wikipedia.org/wiki/ENIAC#:~:text=ENIAC%20(%2F%CB%88%C9%9Bni,of%20numerical%20problems%22%20through%20
    reprogramming">[5]</a> “ENIAC.” Wikipedia, Wikimedia Foundation, 18 Jan. 2021, en.wikipedia.org/wiki/ENIAC#:~:text=ENIAC%20(%2F%CB%8
    8%C9%9Bni,of%20numerical%20problems%22%20through%20reprogramming. 
</p>
<p>
    <a href="https://en.wikipedia.org/wiki/Harvard_Mark_I">[6]</a> “Harvard Mark I.” Wikipedia, Wikimedia Foundation, 17 Jan. 2021,
    en.wikipedia.org/wiki/Harvard_Mark_I. 
</p>
<p>
    <a href="https://en.wikipedia.org/wiki/History_of_general-purpose_CPUs#Mid-to-late_1980s:_Exploiting_instruction_level_parallelism">[7]</a>
    “History of General-Purpose CPUs.” Wikipedia, Wikimedia Foundation, 17 Jan. 2021, en.wikipedia.org/wiki/History_of_general-purpose_CPUs#Mid
    -to-late_1980s:_Exploiting_instruction_level_parallelism. 
</p>
<p>
    <a href="https://en.wikipedia.org/wiki/Intel_4004">[8]</a> “Intel 4004.” Wikipedia, Wikimedia Foundation, 25 Dec. 2020,
    en.wikipedia.org/wiki/Intel_4004. 
</p>
<p>
    <a href="https://www.zdnet.com/article/amd-closing-in-on-intel-in-desktop-cpu-market-share/#:~:text=For%20the%20first%20time%20since,
    with%20Intel%20at%2050.2%20percent">[9]</a> Kingsley-Hughes, Adrian. “AMD Closing in on Intel in Desktop CPU Market Share.” ZDNet,
    ZDNet, 5 Jan. 2021, www.zdnet.com/article/amd-closing-in-on-intel-in-desktop-cpu-market-share/#:~:text=For%20the%20first%20time%20
    since,with%20Intel%20at%2050.2%20percent. 
</p>
<p>
    <a href="https://en.wikipedia.org/wiki/Moore%27s_law">[10]</a> “Moore's Law.” Wikipedia, Wikimedia Foundation, 17 Jan. 2021,
    en.wikipedia.org/wiki/Moore%27s_law. 
</p>
<p>
    <a href="https://en.wikipedia.org/wiki/Personal_computer#History">[11]</a> “Personal Computer.” Wikipedia, Wikimedia Foundation,
    21 Jan. 2021, en.wikipedia.org/wiki/Personal_computer#History. 
</p>
<p>
    <a href="https://en.wikipedia.org/wiki/Von_Neumann_architecture">[12]</a> “Von Neumann Architecture.” Wikipedia, Wikimedia Foundation,
    19 Jan. 2021, en.wikipedia.org/wiki/Von_Neumann_architecture. 
</p>
</body>

</html>